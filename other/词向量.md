理解 测试数据集特点

理解word2vec论文

分布式词向量：混淆语法相关和语义相关

因此，分布向量之间的相似性仅表示抽象语义关联，而不是精确的语义关系，For example, it is difficult to discern
synonyms from antonyms in distributional spaces.

Specialization models：external lexical knowledge(wordNet, BabelNet)

- 在目标函数中加入约束
- finetune通用词向量，满足约束



### 语言模型

### 基于predictive的word2vec

cbow：这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。

Skip-Gram：和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为4， 特定的这个词"Learning"是我们的输入，而这8个上下文词是我们的输出。这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的8个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前8的softmax概率对应的神经元所对应的词即可。



**三层神经网络，最后一层采用huffman预测（为什么）**

Hierarchical Softmax：那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1.如上图，则可以得到c的编码是00。

和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为"Hierarchical Softmax"。

**基于Hierarchical Softmax的CBOW模型**

在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。

对于从输入层到隐藏层（投影层），这一步比较简单，就是对ww周围的2c2c个词向量求和取平均即可,中间结点没有实际物理意义，只是辅助向量

arg min log似然

**基于Hierarchical Softmax的Skip-Gram模型**

现在我们先看看基于Skip-Gram模型时， Hierarchical Softmax如何使用。此时输入的只有一个词ww,输出的为2c2c个词向量context(w)context(w)。

我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的cc个词和后面的cc个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。Skip-Gram模型和CBOW模型其实是反过来的，在上一篇已经讲过。

在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。

对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即xwxw就是词ww对应的词向量。

第二步，通过梯度上升法来更新我们的θwj−1θj−1w和xwxw，注意这里的xwxw周围有2c2c个词向量，此时如果我们期望P(xi|xw),i=1,2...2cP(xi|xw),i=1,2...2c最大。此时我们注意到由于上下文是相互的，在期望P(xi|xw),i=1,2...2cP(xi|xw),i=1,2...2c最大化的同时，反过来我们也期望P(xw|xi),i=1,2...2cP(xw|xi),i=1,2...2c最大。那么是使用P(xi|xw)P(xi|xw)好还是P(xw|xi)P(xw|xi)好呢，word2vec使用了后者，这样做的好处就是在一个迭代窗口内，我们不是只更新xwxw一个词，而是xi,i=1,2...2cxi,i=1,2...2c共2c2c个词。这样整体的迭代会更加的均衡。因为这个原因，Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对2c2c个输出进行迭代更新。

在讲基于Negative Sampling的word2vec模型前，我们先看看Hierarchical Softmax的的缺点。的确，使用霍夫曼树来代替传统的神经网络，可以提高模型训练的效率。但是如果我们的训练样本里的中心词ww是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？

Negative Sampling就是这么一种求解word2vec模型的方法，它摒弃了霍夫曼树，采用了Negative Sampling（负采样）的方法来求解，下面我们就来看看Negative Sampling的求解思路。

### 基于count-base的Glove

#### 概念

* X词共现矩阵
* $X_{ij}$词对 