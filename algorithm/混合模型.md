## 混合模型和EM

### K-Means聚类

首先，我们考虑寻找多维空间中数据点的分组或聚类的问题。假设我们有一个数据集$\{x_1, . . . , x_N \} $ ，它由$D$维欧几里得空间中的随机变量$x$ 的$N$ 次观测组成。我们的目标是将数据集划分为$K$个类别。引入一组D维向量$\mu_k$ ，其中$k = 1, . . . , K$ ，且$\mu_k$是与第$k$个聚类关联的一个代表。正如我们将看到的那样，我们可以认为$\mu_k$ 表示了聚类的中心。我们的目标是找到数据点分别属于的聚类，以及一组向量$\{\mu_k \}$，使得每个数据点和与它最近的向量$\mu_k$ 之间的距离的平方和最小。

对于每个数据点$x_n$ ，引入一组对应的二值指示变量$r_{nk} \in \{0,1\}$ ,其中$k=1,...,K$表示数据点$x_n$ 属于$K$个聚类中
的哪一个，从而如果数据点$x_n$ 被分配到类别$k$，那么$r_{nk} = 1$ ，且对于$j  \neq k$ ，有$r_{nj} = 0$ 。目标函数：
$$
J=\sum^N_{n=1}\sum_{k=1}^Kr_{nk}\|x_n-\mu_k\|^2
$$
它表示每个数据点与它被分配的向量$\mu_k$之间的距离的平方和。我们的目标是找到$\{r_{nk} \}$和$\{\mu_k \}$的值，使得$J$ 达到最小值。 首先，我们为$\mu_k$选择一些初始值。然后，在第一阶段，我们关于$r_{nk}$最小化$J$ ，保持$\mu_k$固定。在第二阶段，我们关于$\mu_k$最小化$J$ ，保持$r_{nk}$固定。更新$r_{nk}$ 和更新$\mu_k$的两个阶段分别对应于EM算法中的E(期望)步骤和M(最大化)步骤。

**首先考虑确定$r_{nk}$。**$J$ 是$r_{nk}$的一个线性函数，我们可以对每个$n$分别进行最优化，只要$k$的值使$\|x_n − \mu_k\|^2$最小，我们就令$r_{nk}$等于1。**即寻找距离最小的聚类中心，将这个点赋予这个类。**

**现在考虑$r_{nk}$固定时，关于$\mu_k$的最优化。**目标函数$J$是$\mu_k$的一个二次函数，令它关于$\mu_k$的导数等于零:
$$
\begin{align}\begin{split}\sum_{n=1}^Nr_{nk}(x_n-\mu_k)&=0\\
\mu_k=\frac{\sum_n r_{nk}x_n}{\sum_n r_{nk}}
\end{split}\end{align}
$$

### 混合高斯

高斯混合概率分布可以写成高斯分布的线性叠加的形式:
$$
p(\boldsymbol{x})=\sum^K_{k=1}\pi_k\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})
$$
让我们引入一个$K$为二值随机变量$\boldsymbol{z}$ ，其中一个特定的元素$z_k$等于1，其余所有的元素等于0。于是$z_k$的值满足$z_k \in \{0, 1\}$且$\sum_k z_k = 1$ ，并且我们看到根据哪个元素非零，向量$\boldsymbol{z}$有$K$个可能的状态。我们根据边缘概率分布$p(\boldsymbol{z})$和条件概率分布$p(\boldsymbol{x}|\boldsymbol{z})$定义联合概率分布$p(\boldsymbol{x},\boldsymbol{z})$。$\boldsymbol{z}$的边缘概率分布根据混合系数$\pi_k$进行赋值，$p(z_k=1)=\pi_k,0\leq\pi_k\leq1,\sum_{k=1}^K\pi_k=1$ 。

我们也可以将这个概率分布写成$p(\boldsymbol{z})=\prod_{k=1}^K\pi_k^{z_k}$。类似地，给定$\boldsymbol{z}$的一个特定的值，$\boldsymbol{x}$ 的条件概率分布是一个高斯分布：

$$
p(\boldsymbol{x}|z_k=1)=\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)
$$
也可以写成:
$$
p(\boldsymbol{x}|\boldsymbol{z})=\prod_{k=1}^K\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)^{z_k}
$$
联合概率分布为$p(\boldsymbol{z})p(\boldsymbol{x}|\boldsymbol{z})$，从而的$\boldsymbol{x}$边缘概率分布可以通过将联合概率分布对所有可能的$\boldsymbol{z}$求和的方式得到，即 
$$
p(\boldsymbol{x})=\sum_\boldsymbol{z}p(\boldsymbol{z})p(\boldsymbol{x}|\boldsymbol{z})=\sum^K_{k=1}\pi_k\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})
$$
如果我们有若干个观测$\boldsymbol{x}_1, . . . , \boldsymbol{x}_N$，那么，由于我们已经用$p(\boldsymbol{x})=\sum_\boldsymbol{z}p(\boldsymbol{z})p(\boldsymbol{x}|\boldsymbol{z})$的方式表示了边缘概率分布，因此对于每个观测数据点$\boldsymbol{x}_n$，存在一个对应的潜在变量$\boldsymbol{z}_n$。于是，我们找到了高斯混合分布的一个等价的公式，将潜在变量显式地写出。似乎我们这么做没有什么意义。但是，我们现在能够对联合概率分布$p(\boldsymbol{x},\boldsymbol{z})$操作，而不是对边缘概率分布$p(\boldsymbol{x})$操作，这会产生极大的计算上的简化。另一个起着重要作用的量是给定$\boldsymbol{x}$的条件下，$\boldsymbol{z}$的条件概率。我们会用$\gamma(z_k)$表示$p(z_k=1|\boldsymbol{x})$，它的值可以使用贝叶斯定理求出:
$$
\gamma(z_{k})\equiv p(z_k=1|\boldsymbol{x})=\frac{p(z_k=1)p(\boldsymbol{x}|z_k=1)}{\sum^K_{j=1}p(z_j=1)p(\boldsymbol{x}|z_j=1)}=
\frac{\pi_k\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})}{\sum_{j=1}^K\pi_j\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_j},\boldsymbol{\Sigma_j})}
\label{yzk}
$$
我们将$\pi_k$看成$z_k = 1$的先验概率，将$γ(z_{k})$看成观测到$\boldsymbol{x}$之后对应的后验概率。

#### 最大似然

假设我们有一个观测的数据集$\{\boldsymbol{x}_1, . . . , \boldsymbol{x}_N \}$，我们希望使用混合高斯模型来对数据进行建模。我们可以将这个数据集表示为一个$N × D$的矩阵$\boldsymbol{X}$，其中第$n$行为$\boldsymbol{x}^T_n$ 。类似地，对应的隐含变量会被表示为一个$N × K$的矩阵$\boldsymbol{Z}$，它的行为$z_n^T $。对数似然函数为：
$$
ln\space p(\boldsymbol{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma})=\sum_{n=1}^Nln\left\{\sum^K_{k=1}\pi_k\mathcal{N}(\boldsymbol{x_n}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})\right\}\label{con:mle}
$$
**奇异性：**可能存在某个高斯分量退化到一个具体的数据点，造成病态解。

**可区分问题：**对于任意给定的最大似然解，一个由$K$个分量混合而成的概率分布总共会有$K!$ 个等价的解，对应于$K!$种将$K$个参数集合分配到$K$个分量上的方式。换句话说，对于参数值空间中任意给定的点，都会有$K! − 1$个其他的点给出完全相同的概率分布。

最大化高斯混合模型的对数似然函数$\eqref{con:mle}$比单一的高斯分布的情形更加复杂。困难来源于在公式$\eqref{con:mle}$中，对$k$的求和出现在对数计算内部，从而对数函数不再直接作用于高斯分布。如果我们令对数似然函数的导数等于零，那么我们不会得到一个解析解。

#### 用于混合高斯的EM

令公式$\eqref{con:mle}$中$ln\space p(\boldsymbol{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma})$ 关于高斯分量的均值$\boldsymbol{\mu}_k$的偏导等于零，我们有
$$
0=\sum_{n=1}^N\frac{\pi_k\mathcal{N}(\boldsymbol{x_n}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})}{\underbrace{\sum_j\pi_j\mathcal{N}(\boldsymbol{x_n}|\boldsymbol{\mu_j},\boldsymbol{\Sigma_j})}_{\gamma(z_{nk})}}\Big[\sum_k(\boldsymbol{x}_n-\boldsymbol{\mu}_k)\Big]^{-1}
$$
两侧同时乘以$\Sigma_k$(假设矩阵是非奇异的)，整理可得
$$
\boldsymbol{\mu}_k=\frac1{N_k}\sum_{n=1}^N\gamma(z_{nk})\boldsymbol{x}_n\label{muk}
$$
其中$N_k=\sum_{n=1}^N\gamma(z_{nk})$。

令$ln\space p(\boldsymbol{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma})$ 关于$\boldsymbol{\Sigma}_k$的偏导等于零，然后用一个类似的推理过程，使用单一高斯分布协方差矩阵的最大似然结果，我们有
$$
\boldsymbol{\Sigma}_k=\frac1{N_k}\sum_{n=1}^N\gamma(z_{nk})(\boldsymbol{x}_n-\boldsymbol{\mu}_k)(\boldsymbol{x}_n-\boldsymbol{\mu}_k)^T\label{sigmak}
$$
我们关于混合系数$\pi_k$最大化$ln\space p(\boldsymbol{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma})$ ，必须考虑限制条件混合系数的加和等于1。使用拉格朗日乘数法，最大化：
$$
ln\space p(\boldsymbol{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma})+\lambda(\sum_{k=1}^K\pi_k-1)
$$
可得：
$$
0=\sum_{n=1}^N\frac{\mathcal{N}(\boldsymbol{x_n}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})}{\sum_j\pi_j\mathcal{N}(\boldsymbol{x_n}|\boldsymbol{\mu_j},\boldsymbol{\Sigma_j})}+\lambda
$$
将两侧同时乘$\pi_k$ ，然后用$\sum_{k=1}^K\pi_k=1$对$k$求和，会发现$\lambda=-N$，消去$\lambda$，可得：
$$
\pi_k=\frac{N_k}N\label{pik}
$$

值得强调的是结果$\eqref{muk}$、$\eqref{sigmak}$和$\eqref{pik}$并没有给出混合模型参数的一个解析解，因为$\gamma(z_{nk})$通过公式$\eqref{yzk}$以一种复杂的方式依赖于这些参数。然而，这些结果确实给出了一个简单的迭代方法来寻找问题的最大似然解。正如我们将看到的那样，这个迭代过程是EM算法应用于高斯混合模型的一个实例。我们首先为均值、协方差、混合系数选择一个初始值。然后，我们交替进行两个更新，被称为E步骤和M步骤，原因稍后会看到。在期望步骤(expectation step)或者E步骤中，我们使用参数的当前值计算公式$\eqref{yzk}$给出的后验概率。然后，我们将计算出的概率用于最大化步骤(maximization step)或者M步骤中，使用公式$\eqref{muk}$、$\eqref{sigmak}$和$\eqref{pik}$重新估计均值、方差和混合系数。注意，在进行这一步骤时，我们首先使用公式$\eqref{muk}$计算新的均值，然后使用新的均值通过公式$\eqref{sigmak}$找到协方差，这与单一高斯分布的对应结果保持一致。我们稍后会证明，每次通过E步骤和接下来的M步骤对参数的更新确保了对数似然函数的增大。在实际应用中，当对数似然函数的变化量或者参数的变化量低于某个阈值时，我们就认为算法收敛。

**算法：**给定一个高斯混合模型，目标是关于参数(均值、协方差、混合系数)最大化似然函数。

* 初始化均值$\boldsymbol{\mu}_k$、协方差$\boldsymbol{\Sigma}_k$和混合系数$\pi_k$，计算对数似然函数的初始值。
* E步骤。使用当前参数值计算后验概率

$$
\gamma(z_{nk})=
\frac{\pi_k\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})}{\sum_{j=1}^K\pi_j\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu_j},\boldsymbol{\Sigma_j})}
$$

* M步骤。使用当前的$\gamma(z_{nk})$重新估计参数

$$
\begin{align}\begin{split}
\boldsymbol{\mu}_k^{new}&=\frac1{N_k}\sum_{n=1}^N\gamma(z_{nk})\boldsymbol{x}_n\\
\boldsymbol{\Sigma}_k^{new}&=\frac1{N_k}\sum_{n=1}^N\gamma(z_{nk})(\boldsymbol{x}_n-\boldsymbol{\mu}_k)(\boldsymbol{x}_n-\boldsymbol{\mu}_k)^T\\
\pi_k^{new}&=\frac{N_k}N
\end{split}\end{align}
$$

其中$N_k=\sum_{n=1}^N\gamma(z_{nk})$。

* 计算对数似然函数

$$
ln\space p(\boldsymbol{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma})=\sum_{n=1}^Nln\left\{\sum^K_{k=1}\pi_k\mathcal{N}(\boldsymbol{x_n}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})\right\}
$$

检查参数或者对数似然函数的收敛性。如果没有满足收敛的准则，则返回第2步。

### 另一种观点

EM算法的目标是找到具有潜在变量的模型的最大似然解。我们将所有观测数据的集合记作$\boldsymbol{X}$，其中第$n$ 行表示$x^T_n$ 。类似地，我们将所有潜在变量的集合记作$\boldsymbol{Z}$，对应的行为$z^T_n$ 。所有模型参数的集合被记作$\theta$，因此对数似然函数为
$$
ln \space p(\boldsymbol{X}|\boldsymbol{\theta})=ln\{ \sum_{\boldsymbol{Z}}p(\boldsymbol{X},\boldsymbol{Z}|\boldsymbol{\theta})\}
$$
一个关键的现象是，对于潜在变量的求和位于对数的内部。即使联合概率分布$p(\boldsymbol{X},\boldsymbol{Z}|\boldsymbol{\theta})$属于指数族分布，由于这个求和式的存在，边缘概率分布$p(\boldsymbol{X}|\boldsymbol{\theta})$通常也不是指数族分布。求和 式的出现阻止了对数运算直接作用于联合概率分布，使得最大似然解的形式更加复杂。 

现在假定对于$\boldsymbol{X}$中的每个观测，我们都有潜在变量的$\boldsymbol{Z}$对应值。我们将$\{\boldsymbol{X},\boldsymbol{Z}\}$称为完整 (complete)数据集，并且我们称实际的观测数据集$\boldsymbol{X}$是不完整的(incomplete)。完整数据集的对数似然函数的形式为$ln\space p(\boldsymbol{X},\boldsymbol{Z}|\boldsymbol{\theta})$，并且我们假定对这个完整数据的对数似然函数进行最大化是很容易的。












