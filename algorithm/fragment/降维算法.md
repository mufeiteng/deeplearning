## SVD分解

### 回顾特征值和特征向量

我们首先回顾下特征值和特征向量的定义：$Ax=\lambda x$。其中$A$是一个$n×n$的矩阵，$x$是一个$n$维向量，则我们说$\lambda$是矩阵$A$的一个特征值，而$x$是矩阵$A$的特征值$\lambda$所对应的特征向量。如果我们求出了矩阵$A$的$n$个特征值$\lambda_1 \leq \lambda_2 \leq ... \leq \lambda_n$,以及这$n$个特征值所对应的特征向量$\{w_1,w_2,...w_n\}$，如果这$n$个特征向量线性无关，那么矩阵$A$就可以用下式的特征分解表示：
$$
A=W\Sigma W^{-1}
$$
其中$W$是这$n$个特征向量所组成的$n×n$维矩阵，而$\Sigma$为这$n$个特征值为主对角线的$n×n$维矩阵。

### SVD的定义

$SVD$也是对矩阵进行分解，但是和特征分解不同，$SVD$并不要求要分解的矩阵为方阵。假设我们的矩阵$A$是一个$m×n$的矩阵，那么我们定义矩阵$A$的$SVD$为：
$$
A = U\Sigma V^T
$$
其中$U$是一个$m×m$的矩阵，$\Sigma$是一个$m×n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，$V$是一个$n×n$的矩阵。$U$和$V$都是正交矩阵。

那么我们如何求出$SVD$分解后的$U,\Sigma,V$这三个矩阵呢？

对矩阵$A^TA$做特征值分解：
$$
(A^TA)v_i = \lambda_i v_i
$$
将$A^TA$的所有特征向量张成一个$n×n$ 的矩阵$V$，就是我们$SVD$ 公式里面的$V$ 矩阵了。一般我们将$V$中的每个特征向量叫做$A$的右奇异向量。

对矩阵$AA^T$做特征值分解：
$$
(AA^T)u_i = \lambda_i u_i
$$
将$AA^T$的所有特征向量张成一个$m×m$ 的矩阵$U$，就是我们$SVD$ 公式里面的$U$ 矩阵了。一般我们将$U$中的每个特征向量叫做$A$的左奇异向量。

$U$和$V$我们都求出来了，现在就剩下奇异值矩阵$\Sigma$没有求出了。由于$\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\sigma$就可以了。
$$
A=U\Sigma V^T \Rightarrow AV=U\Sigma V^TV \Rightarrow AV=U\Sigma \Rightarrow  Av_i = \sigma_i u_i  \Rightarrow  \sigma_i =  Av_i / u_i
$$
这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵$\Sigma$。

## PCA

### 预备知识

样本$X$和样本$Y$的**协方差**：
$$
Conv(X,Y)=\frac{\sum^{n}_{i=1}(X_i-\bar{X})(Y_i-\bar{Y})}{(n-1)}
$$
比如对于3维数据$(x,y,z)$，计算它的协方差矩阵就是：
$$
C=\begin{bmatrix} cov(x,x) & cov(x,y) & cov(x,z) \\ cov(y,x) & cov(y,y) & cov(y,z) \\ cov(z,x) & cov(z,y) & cov(z,z) \end{bmatrix}
$$

### 过程

将原始数据按列组成$n$行$m$列矩阵$X$, $X$的每一行代表一个属性字段

- 1.特征中心化。即每一维的数据都减去该维的均值。这里的“维”指的就是一个特征（或属性），变换之后每一维的均值都变成了0;
- 计算$X$的协方差矩阵$C=\frac{1}{n-1}X^TX$;
- 求出协方差矩阵$C$的特征值及对应的特征向量;
- 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前$k$行组成矩阵$P$;
- $Y=PX$即为降维到$k$维后的数据

### 解释： 最大方差理论

在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。如前面的图，样本在$u_1$上的投影方差较大，在$u_2$上的投影方差较小，那么可认为$u_2$上的投影是由噪声引起的。因此我们认为，最好的$k$维特征是将$n$维样本点转换为$k$维后，每一维上的样本方差都很大。

## LDA线性判别分析

LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。LDA的基本思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点中心尽可能远离。更简单的概括为一句话，就是“投影后类内方差最小，类间方差最大”。

可能还是有点抽象，我们先看看最简单的情况。假设我们有两类数据分为 “+”和“-”，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而“+”和“-”数据中心之间的距离尽可能的大。

<img src="/Users/aszzy/Documents/study/note/pictures/algorithm/3.jpg" height="200px">

我们将这个最佳的向量称为$w$，那么样例$x$到方向向量$w$上的投影可以用下式来计算: $y=w^{\top}x$ 

当$x$是二维的，我们就是要找一条直线（方向为$w$）来做投影，然后寻找最能使样本点分离的直线。接下来我们从定量的角度来找到这个最佳的$ w$ 。

给定数据集 $D=\{(x_i,y_i)\}_{i=1}^{m}$  ， $y_i\in\{0,1\}$ ，令 $N_i$ 、$X_i$ 、 $\mu_i$ 、 $\Sigma_i$ 分别表示第 $i\in\{0,1\}$ 类示例的样本个数、样本集合、均值向量、协方差矩阵。

$\mu_i$ 的表达式： $\mu_i=\frac{1}{N_i}\sum_{x\in X_i}{x} ， (i=0,1)$； 

$\Sigma_i$ 的表达式： $\Sigma_i=\sum_{x\in X_i}({x-\mu_i})({x-\mu_i})^{\top} ，(i=0,1)$

由于是两类数据，因此我们只需要将数据投影到一条直线上即可。假设我们的投影直线是向量 $w$，则对任意一个样本 $x_i$ ,它在直线 $w$ 的投影为 $w^{\top}x_i$ ,对于我们的两个类别的中心点 $u_0,u_1$ ，在直线 $w$ 的投影为 $w^{\top}\mu_0$ 和 $w^{\top}\mu_1$ ，分别用 $\tilde{\mu_0}$ 和 $\tilde{\mu_1}$ 来表示。

什么是最佳的 $w$ 呢？我们首先发现，**能够使投影后的两类样本中心点尽量分离的直线是好的直线**，定量表示就是：
$$
arg\max_w J(w)=||w^{\top}\mu_0-w^{\top}\mu_1||_2^2=||\tilde{\mu_0}-\tilde{\mu_1}||_2^2
$$
**但是只考虑 $J(w)$ 行不行呢？**不行，看下图:

<img src="/Users/aszzy/Documents/study/note/pictures/algorithm/4.png" height="200px">

样本点均匀分布在椭圆里，投影到横轴$x_1$上时能够获得更大的中心点间距$J(w)$，但是由于有重叠，$x_1$不能分离样本点。投影到纵轴$x_2$上，虽然$J(w)$较小，但是能够分离样本点。**因此我们还需要考虑同类样本点之间的方差，同类样本点之间方差越小， 就越难以分离**。

我们引入另外一个度量值，称作散列值（ scatter），对**投影后的类**求散列值，如下:
$$
\tilde{S}^2=\sum_{x\in X_i}(w^\top x-\tilde{\mu_i})^2
$$
从公式中可以看出，只是少除以样本数量的方差值，散列值的几何意义是样本点的密集程度，值越大，越分散；反之，越集中。

而我们想要的投影后的样本点的样子是：**不同类别的样本点越分开越好，同类的越聚集越好**，**也就是均值差越大越好，散列值越小越好**。 正好，我们同时考虑使用$J(w)$和$S$来度量，则可得到欲最大化的目标：
$$
J(w)=\frac{||w^{\top}\mu_0-w^{\top}\mu_1||^2_2}{\tilde{S_0}^2+\tilde{S_1}^2}
$$
接下来的事就比较明显了，我们只需寻找使$J(w)$最大的$w$即可。

先把散列值公式展开:
$$
\tilde{S}^2=\sum_{x\in X_i}(w^\top x-\tilde{u_i})^2=\sum_{x\in X_i}w^{\top}(x-\mu_i)(x-\mu_i)^{\top}w
$$
我们定义上式中的中间那部分：
$$
\Sigma_i=\sum_{x\in X_i}(x-\mu_i)(x-\mu_i)^{\top}
$$
这个公式的样子不就是少除以样例数的协方差矩阵么，称为散列矩阵（ scatter matrices）。

我们继续定义“**类内散度矩阵**”（within-class scatter matrix）：
$$
S_w=\Sigma_0+\Sigma_1
$$
以及“**类间散度矩阵**”（between-class scatter matrix)：
$$
S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^\top
$$

$$
||w^{\top}\mu_0-w^{\top}\mu_1||^2_2=w^{\top}(\mu_0-\mu_1)(\mu_0-\mu_1)^{\top}w
$$

则$J(w)$可以重写为：
$$
J(w)=\frac{w^{\top}S_bw}{w^{\top}S_ww}
$$
这就是LDA欲最大化的目标。即 $S_b$ 与 $S_w$的“广义瑞利商”(generalized Rayleigh quotient)。

**如何确定 $w$呢？**

注意到 $J(w)$ 式中的分子和分母都是关于$w$的二次项，因此$J(w)$的解与$w$的长度无关，只与其方向有关（$w$为投影后直线的方向），不失一般性，令 $w^{\top}S_ww=1$ ，则式 $J(w)$ 等价于：
$$
\begin{align}\begin{split} &\min_{w} -w^{\top}S_bw \\ & s.t.  w^{\top}S_ww=1 
\end{split} \end{align}
$$
由拉格朗日乘子法，上式等价于：
$$
\begin{align}\begin{split}
c(w) &=-w^{\top}S_bw+\lambda(w^{\top}S_ww-1)\\ &\Rightarrow\frac{d{c}}{d{w}}=-2S_bw+2\lambda S_ww=0\\ &\Rightarrow S_bw=\lambda S_ww \end{split} \end{align}
$$
其中$\lambda$为拉格朗日乘子。注意到 $S_bw$ 的方向恒为 $\mu_0-\mu_1$ ，不妨令：$S_bw=\lambda(\mu_0-\mu_1)$

将其带入上式得：$ w=S_w^{-1}(\mu_0-\mu_1)$ 

也就是说我们只要求出原始二类样本的均值和方差就可以确定最佳的投影方向$w$了。

### 降维总结

PCA（主成分分析）和LDA（线性判别分析）有很多的相似点，其本质是要将初始样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。LDA算法既可以用来降维，又可以用来分类，但是目前来说，主要还是用于降维。

