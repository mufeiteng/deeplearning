## EM

### 预备知识

#### 极大似然估计

假设我们需要调查我们学校学生的身高分布。我们先假设学校所有学生的身高服从正态分布 $N(\mu,\sigma^2)$。(**注意：极大似然估计的前提一定是要假设数据总体的分布，如果不知道数据分布，是无法使用极大似然估计的**)，这个分布的均值 $\mu$和方差 $\sigma^2$未知，如果我们估计出这两个参数，那我们就得到了最终的结果。那么怎样估计这两个参数呢？

我们可以先对学生进行抽样。假设我们随机抽到了 200 个人（也就是 200 个身高的样本数据，为了方便表示，下面，“人”的意思就是对应的身高）。然后统计抽样这 200 个人的身高。根据这 200 个人的身高估计均值 $\mu$和方差 $\sigma^2$ 。

用数学的语言来说就是：为了统计学校学生的身高分布，我们独立地按照概率密度 $p(x|θ)$抽取了 200 个（身高），组成样本集  $X={x_1,x_2,…,x_N}$(其中$x_i$表示抽到的第 $i$个人的身高，这里 N 就是 200，表示样本个数)，我们想通过样本集 X 来估计出未知参数 $\theta$。这里概率密度  $p(x|θ)$服从高斯分布 $N(\mu,\sigma^2)$，其中的未知参数是 $θ=[\mu, \sigma]^T$。        

那么问题来了怎样估算参数 $\theta$呢？

我们先回答几个小问题：

**问题一：抽到这 200 个人的概率是多少呢？**

由于每个样本都是独立地从 $p(x|θ)$ 中抽取的，换句话说这 200 个学生随便捉的，他们之间是没有关系的，即他们之间是相互独立的。假如抽到学生 A（的身高）的概率是 $ p(x_A|θ)$, 抽到学生B的概率是 $p(x_B|θ)$，那么同时抽到男生 A 和男生 B 的概率是 $p(x_A|θ) \times p(x_B|θ)$，同理，我同时抽到这 200 个学生的概率就是他们各自概率的乘积了，即为他们的联合概率，用下式表示：
$$
L(\theta) = L(x_1, x_2, \cdots , x_n; \theta) = \prod_{i=1}^{n}p(x_i|\theta), \quad \theta \in \Theta
$$
$n$为抽取的样本的个数，本例中 $n=200$ ，这个概率反映了，在概率密度函数的参数是$\theta$时，得到$X$这组样本的概率。因为这里 L 是已知的，也就是说我抽取到的这 200 个人的身高可以测出来。而$\theta$ 是未知了，则上面这个公式只有$\theta$是未知数，所以 L 是的$\theta$函数。这个函数反映的是在不同的参数$\theta$ 取值下，取得当前这个样本集的可能性，因此称为参数$\theta$ 相对于样本集 X 的似然函数，记为$\theta$ 。

为了便于分析，对 L 取对数，将其变成连加的，称为对数似然函数，如下式：
$$
H(\theta) = \text{ln} \ L(\theta) = \text{ln} \prod \limits _{i=1}^{n}p(x_i|\theta) = \sum \limits _{i=1}^{n}\text{ln} p(x_i|\theta)
$$
**问题二：学校那么多学生，为什么就恰好抽到了这 200 个人 ( 身高) 呢？**

在学校那么学生中，我一抽就抽到这 200 个学生（身高），而不是其他人，那是不是表示在整个学校中，这 200 个人（的身高）出现的概率极大啊，也就是其对应的似然函数 $L(θ)$极大，即
$$
\hat \theta = \text{argmax} \ L(\theta)
$$
**问题三：那么怎么极大似然函数？**

求 $L(\theta)$对所有参数的偏导数，然后让这些偏导数为 0，假设有$n$个参数，就有$n$个方程组成的方程组，那么方程组的解就是似然函数的极值点了，从而得到对应的$\theta$了。

#### Jensen不等式

设$f$是定义域为实数的函数，如果对于所有的实数$x$，$f(x)$的二次导数大于等于0，那么$f$是凸函数。 

**Jensen不等式**: 如果$f$是凸函数，$X$是随机变量，那么：$E[f(X)]\ge f(E[X]) $ 。当且仅当$X$是常量时，上式取等号。



### 传统EM算法详述

#### 问题描述

上面我们先假设学校所有学生的身高服从正态分布 $N(\mu,\sigma^2)$ 。实际情况并不是这样的，男生和女生分别服从两种不同的正态分布，即$boy \in N(\mu_1, \sigma_1^2)$，$ girl \in N(\mu_2, \sigma_2^2)$ ，(**注意：EM算法和极大似然估计的前提是一样的，都要假设数据总体的分布，如果不知道数据分布，是无法使用EM算法的**)。那么该怎样评估学生的身高分布呢？

简单啊，我们可以随便抽 100 个男生和 100 个女生，将男生和女生分开，对他们单独进行极大似然估计。分别求出男生和女生的分布。

假如某些男生和某些女生好上了，纠缠起来了。咱们也不想那么残忍，硬把他们拉扯开。这时候，你从这 200 个人（的身高）里面随便给我指一个人（的身高），我都无法确定这个人（的身高）是男生（的身高）还是女生（的身高）。用数学的语言就是，抽取得到的每个样本都不知道是从哪个分布来的。那怎么办呢？

这个时候，对于每一个样本或者你抽取到的人，就有两个问题需要估计了，一是这个人是男的还是女的，二是男生和女生对应的身高的正态分布的参数是多少。

#### 过程

但是现在我们既不知道每个学生是男生还是女生，也不知道男生和女生的身高分布。这就成了一个先有鸡还是先有蛋的问题了。鸡说，没有我，谁把你生出来的啊。蛋不服，说，没有我，你从哪蹦出来啊。为了解决这个你依赖我，我依赖你的循环依赖问题，总得有一方要先打破僵局，不管了，我先随便整一个值出来，看你怎么变，然后我再根据你的变化调整我的变化，然后如此迭代着不断互相推导，最终就会收敛到一个解。这就是EM算法的基本思想了。

EM的意思是“**Expectation Maximization**”，具体方法为：

- 先设定男生和女生的身高分布参数(初始值)，例如男生的身高分布为 $N(\mu_1 = 172, \sigma^2_1=5^2)$， 女生的身高分布为 $N(\mu_2 = 162, \sigma^2_2=5^2)$，当然了，刚开始肯定没那么准；
- 然后计算出每个人更可能属于第一个还是第二个正态分布中的（例如，这个人的身高是180，那很明显，他极大可能属于男生），这个是属于Expectation 一步；
- 我们已经大概地按上面的方法将这 200 个人分为男生和女生两部分，我们就可以根据之前说的极大似然估计分别对男生和女生的身高分布参数进行估计（这不变成了**极大**似然估计了吗？**极大即为Maximization**）这步称为 Maximization；
- 然后，当我们更新这两个分布的时候，每一个学生属于女生还是男生的概率又变了，那么我们就再需要调整E步；
- ……如此往复，直到参数基本不再发生变化或满足结束条件为止。

#### 总结

上面的学生属于男生还是女生我们称之为隐含参数，女生和男生的身高分布参数称为模型参数。

EM 算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含参数（EM 算法的 E 步），接着基于观察数据和猜测的隐含参数一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐含参数是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。我们基于当前得到的模型参数，继续猜测隐含参数（EM算法的 E 步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。

一个最直观了解 EM 算法思路的是 K-Means 算法。在 K-Means 聚类时，每个聚类簇的质心是隐含数据。我们会假设 K 个初始化质心，即 EM 算法的 E 步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即 EM 算法的 M 步。重复这个 E 步和 M 步，直到质心不再变化为止，这样就完成了 K-Means 聚类。

### 推导

对于 $m$ 个相互独立的样本 $x=(x^{(1)},x^{(2)},...x^{(m)})$ ，**如样本身高，**对应的隐含数据 $z=(z^{(1)},z^{(2)},...z^{(m)})$ ，**如从每个高斯分布中采样的概率，**此时 $(x,z)$即为完全数据，样本的模型参数为 $θ$ , 则观察数据 $x^{(i)}$的概率为 $P(x^{(i)}|\theta)$，完全数据 $(x^{(i)},z^{(i)})$的似然函数为 $P(x^{(i)},z^{(i)}|\theta)$。

假如没有隐含变量$z$ ，我们仅需要找到合适的$\theta$极大化对数似然函数即可：
$$
\theta =arg \max \limits_{\theta}L(\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m logP(x^{(i)}|\theta)
$$
增加隐含变量$z$之后，我们的目标变成了找到合适的$\theta$和$z$让对数似然函数极大：
$$
\theta, z = arg \max \limits_{\theta,z}L(\theta, z) = arg \max \limits_{\theta,z}\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)}|\theta)
$$
如果对分别对未知的$\theta$和$z$分别求偏导，由于$ logP(x^{(i)}|\theta)$是 $P(x^{(i)}， z^{(i)}|\theta)$边缘概率(建议没基础的同学网上搜一下边缘概率的概念)，转化为 $ logP(x^{(i)}|\theta)$求导后形式会非常复杂（可以想象下 $log(f_1(x)+ f_2(x)+…)$复合函数的求导) ，所以很难求解得到 $\theta$和$z$。那么我们想一下可不可以将加号从 log 中提取出来呢？

我们对这个式子进行缩放如下：
$$
\begin{align}\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)},z^{(i)}|\theta) & = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)},z^{(i)}|\theta)}{Q_i(z^{(i)})} \label{addQ} \\
& \geq \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)},z^{(i)}|\theta)}{Q_i(z^{(i)})}\label{jensen}\end{align}
$$
上面第$\eqref{addQ}$式引入了一个未知的新的分布 $Q_i(z^{(i)})$，满足：
$$
\sum \limits _z Q_i(z)=1,0 \le Q_i(z)\le 1 
$$
第$\eqref{jensen}$式用到了 Jensen 不等式 (对数函数是凹函数)：
$$
log(E(y)) \ge E(log(y))
$$
其中：
$$
\begin{align}\begin{split}
E(y) &= \sum\limits_i\lambda_iy_i, \lambda_i \geq 0, \sum\limits_i\lambda_i =1\\
y_i& = \frac{P(x^{(i)}, z^{(i)}|\theta)}{Q_i(z^{(i)})}\\
\lambda_i &= Q_i(z^{(i)})
\end{split}\end{align}
$$

也就是说 $\frac{P(x^{(i)},z^{(i)}|\theta)}{Q_i(z^{(i)})}​$ 为第 $i​$ 个样本*，* $ Q_i(z^{(i)})​$为第 $i​$ 个样本对应的权重，那么：
$$
E(log\frac{P(x^{(i)},z^{(i)}|\theta)}{Q_i(z^{(i)})}) = \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)},z^{(i)}|\theta)}{Q_i(z^{(i)})}
$$
上式我实际上是我们构建了 $L(\theta, z)$ 的下界，我们发现实际上就是 $log\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})}$的加权平均，由于上面讲过权值 $Q_i(z^{(i)})$累积和为1，因此上式是 $log\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})}$的期望，**这就是Expectation的来历啦**。下一步要做的就是寻找一个合适的 $Q_i(z)$最优化这个下界(M步)。

假设$\theta$已经给定，那么 $logL(\theta)$的值就取决于 $Q_i(z)$和 $ p(x^{(i)},z^{(i)})$了。我们可以通过调整这两个概率使下界逼近 $logL(\theta)$的真实值，当不等式变成等式时，说明我们调整后的下界能够等价于$logL(\theta)$了。由 Jensen 不等式可知，等式成立的条件是随机变量是常数，则有：
$$
\frac{P(x^{(i)},z{(i)}|\theta)}{Q_i(z{(i)})} =c 
$$
其中 $c$为常数，对于任意$i$，我们得到：
$$
{P(x^{(i)},z^{(i)}|\theta)} =c{Q_i(z^{(i)})}
$$
方程两边同时累加和：
$$
\sum\limits_{z} {P(x^{(i)},z^{(i)}|\theta)} = c\sum\limits_{z} {Q_i(z^{(i)})}
$$
由于 $\sum\limits_{z}Q_i(z^{(i)}) =1$ ，从上面两式，我们可以得到：
$$
\sum\limits_{z} {P(x^{(i)},z^{(i)}|\theta)} = c
$$

$$
Q_i(z^{(i)}) = \frac{P(x^{(i)},z^{(i)}|\theta)}{c} = \frac{P(x^{(i)},z^{(i)}|\theta)}{\sum\limits_{z}P(x^{(i)},z^{(i)}|\theta)} = \frac{P(x^{(i)},z^{(i)}|\theta)}{P(x^{(i)}|\theta)} = P( z^{(i)}|x^{(i)},\theta) 
$$

其中：

边缘概率公式： $P(x^{(i)}|\theta) = \sum\limits_{z}P(x^{(i)}， z^{(i)}|\theta)$ 

条件概率公式： $\frac{P(x^{(i)}， z^{(i)}|\theta)}{P(x^{(i)}|\theta)} = P( z^{(i)}|x^{(i)}，\theta)$

从上式可以发现 $Q(z)$是已知样本和模型参数下的隐变量分布。

如果 $Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}，\theta))$ , 则第 (9) 式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要极大化下式:
$$
arg \max \limits_{\theta} \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}, z^{(i)}|\theta)}{Q_i(z^{(i)})}
$$
至此，我们推出了在固定参数 $\theta$后分布 $Q_i(z^{(i)})$的选择问题， 从而建立了 $logL(\theta)$ 的下界，这是 E 步，接下来的M 步骤就是固定 $Q_i(z^{(i)})$后，调整 $\theta$，去极大化$logL(\theta)$的下界。

去掉上式中常数的部分 $Q_i(z^{(i)})$，则我们需要极大化的对数似然下界为：
$$
arg \max \limits_{\theta} \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)},z^{(i)}|\theta)}
$$

### EM算法流程

输入：观察数据$x=(x^{(1)},x^{(2)},...x^{(m)})$，联合分布 $p(x,z |\theta)$，条件分布 $p(z|x, \theta)$， 极大迭代次数 $J$。

1) 随机初始化模型参数 $\theta$的初值 $\theta^0$

2) $\text{for j from 1 to J}$ ：

- E步：计算联合分布的条件概率期望：

$$
Q_i(z^{(i)}) := P( z^{(i)}|x^{(i)},\theta))
$$

- M步：极大化 $L(\theta)$  ,得到 $\theta$ :

$$
\theta : = arg \max \limits_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)},z^{(i)}|\theta)} 
$$

- 重复E、M步骤直到 $\theta$收敛

输出：模型参数$\theta$ 

### EM的应用

- 支持向量机的SMO算法
- 混合高斯模型
- K-means
- 隐马尔可夫模型

