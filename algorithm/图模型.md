## 图模型

### 预备知识

#### 概率规则

* 乘积规则：$p(X,Y)=p(Y|X)p(X)$
* 加和规则：$p(X)=\sum_Yp(X,Y)$

#### 贝叶斯概率

使用一个观测数据集来决定概率分布的参数的一个通用的标准是寻找使似然函数取得最大值的参数值。这个标准看起来可能很奇怪，因为从我们之前对于概率论的讨论来看，似乎在给定数据集的情况下最大化概率的参数(而不是在给定参数的情况下最大化数据集出现的概率)是
更加自然的。贝叶斯观点的一个优点是对先验概率的包含是很自然的事情。例如，假定投掷一枚普通的硬币3次，每次都是正面朝上。一个经典的最大似然模型在估计硬币正面朝上的概率时，结果会是1，表示所有未来的投掷都会是正面朝上!相反，一个带有任意的合理的先验的贝叶斯的方法将不会得出这么极端的结论。

#### 多项式曲线拟合

现在假设给定一个训练集，该训练集由x的N次观测组成，写作$\boldsymbol{x}= (x_1, . . . , x_N )^T$，伴随着对应的$t$的观测值，记作$\boldsymbol{t}= (t_1, . . . , t_N )^T $，我们使用下面形式的多项式函数来拟合数据:
$$
y(x,\boldsymbol{w})=w_0+w_1x+w_2x^2+\cdots+w_Mx^M=\sum_{j=1}^Mw_jx^j
$$
系数的值可以通过调整多项式函数拟合训练数据的方式确定。这可以通过最小化误差函数的方法实现。误差函数衡量了对于任意给定的$\boldsymbol{w}$值，函数$y(x, \boldsymbol{w})$与训练集数据的差别。一个简单的应用广泛的误差函数是每个数据点$x_n$的预测值$y(x_n, \boldsymbol{w})$与目标值$t_n$的平方和。所以我们最小化:
$$
E(\boldsymbol{w})=\frac12\sum_{n=0}^N(y(x_n, \boldsymbol{w})-t_n)^2+\frac \lambda2\|\boldsymbol{w}\|^2
$$

#### 重新考察曲线拟合

曲线拟合问题的目标是能够根据$N$个输入$\boldsymbol{x}= (x_1, . . . , x_N )^T$组成的数据集和它们对应的目标值$\boldsymbol{t}= (t_1, . . . , t_N )^T $，在给出输入变量$x$的新值的情况下，对目标变量$t$进行预测。我们可以使用概率分布来表达关于目标变量的值的不确定性。为了达到这个目的，我们要假定，给定$x$的值， 对应的$t$值服从高斯分布，分布的均值为$y(x, \boldsymbol{w})$。因此，我们有 
$$
p(t|x,\boldsymbol{w},\beta)=\mathcal{N}(t|y(x,\boldsymbol{w}),\beta^{-1})
$$
其中，为了和后续章节中的记号相同，我们定义了精度参数$\beta$，它对应于分布方差的倒数$\beta^{-1}=\sigma^2$。

<img src="/Users/aszzy/Documents/study/note/pictures/algorithm/7.png" height="200px">

我们现在用训练数据$\{\boldsymbol{x},\boldsymbol{t}\}$，通过最大似然方法，来决定未知参数$\boldsymbol{w}$和$\beta$的值。如果数据假定从分布(4)中抽取，那么似然函数为
$$
\begin{align}\begin{split}
p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{w},\beta)&=\prod^N_{n=1}\mathcal{N}(t_n|y(x_n,\boldsymbol{w}),\beta^{-1})\\
\Rightarrow ln \space p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{w},\beta)&=-\frac\beta2\sum_{n=1}^N(y(x_n,\boldsymbol{w})-t_n)^2+\frac N2ln\space\beta-\frac N2ln(2\pi)
\end{split}\end{align}
$$
首先考虑确定多项式系数的最大似然解(记作$\boldsymbol{w}_{ML}$)。这些由公式(5)关于$\boldsymbol{w}$来确定。 为了达到这个目的，我们可以省略公式(5)右侧的最后两项，因为他们不依赖于$\boldsymbol{w}$。并且， 我们注意到，使用一个正的常数系数来缩放对数似然函数并不会改变关于$\boldsymbol{w}$的最大值的位置， 因此我们可以用1来代替系数 $\beta$ 。最后，我们不去最大化似然函数，而是等价地去最小化负对数似然函数。于是我们看到，目前为止对于确定$\boldsymbol{w}$的问题来说，最大化似然函数等价于最小化由公式(3)定义的平方和误差函数。因此，在高斯噪声的假设下，平方和误差函数是最大化似然函数的一个自然结果。 

我们也可以使用最大似然方法来确定高斯条件分布的精度参数$\beta$。关于$\beta$来最大化函数 (5)，我们有 
$$
\frac1{\beta_{ML}}=\frac1N\sum_{n=1}^N(y(x_n,\boldsymbol{w}_{ML})-t_n)^2
$$
我们又一次首先确定控制均值的参数向量$\boldsymbol{w}_{ML}$，然后使用这个结果来寻找精度$\beta_{ML}$。这与简单高斯分布时的情形相同。已经确定了参数$\boldsymbol{w}$和$\beta$，我么现在可以对新的x的值进行预测。由于我们现在有一个概率模型，预测可以通过给出$t$的概率分布的预测分布(predictive distribution)来表示(而不仅仅是一个点的估计)。预测分布通过把最大似然参数代入公式(4)给出。
$$
p(t|x,\boldsymbol{w}_{ML},\beta_{ML})=\mathcal{N}(t|y(x,\boldsymbol{w}_{ML}),\beta^{-1}_{ML})
$$
现在让我们朝着贝叶斯的方法前进一步，引入在多项式系数$\boldsymbol{w}$上的先验分布。简单起见，我们考虑下面形式的高斯分布
$$
p(\boldsymbol{w}|\alpha)=\mathcal{N}(\boldsymbol{w}|\boldsymbol{0},\alpha^{-1}\boldsymbol{I})=(\frac{\alpha}{2\pi})^{\frac{M+1}2}exp(-\frac\alpha2\boldsymbol{w}^T\boldsymbol{w})
$$
其中$\alpha$是分布的精度，$M+1$是对于$M$阶多项式的向量$\boldsymbol{w}$的元素的总数。像$\alpha$这样控制模型参数分布的参数，被称为超参数。使用贝叶斯定理，$\boldsymbol{w}$的后验概率正比于先验分布和似然函数的乘积。 
$$
p(\boldsymbol{w}|\boldsymbol{x},\boldsymbol{t},\alpha,\beta)\propto p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{w},\beta)\cdot p(\boldsymbol{w}|\alpha)
$$
给定数据集，我们现在通过寻找最可能的$\boldsymbol{w}$值(即最大化后验概率)来确定$\boldsymbol{w}$。这种技术被称为最大后验(maximum posterior)，简称MAP。我们可以看到，最大化后验概率就是最小化下式:
$$
\frac\beta2\sum_{n=1}^N(y(x_n,\boldsymbol{w})-t_n)^2+\frac \alpha 2 \boldsymbol{w}^T\boldsymbol{w}
$$
因此我们看到最大化后验概率等价于最小化正则化的平方和误差函数，正则化参数为$\lambda=\frac\alpha\beta$ 。

#### 贝叶斯曲线拟合

在一个纯粹的贝叶斯方法中，我们应该自始至终地应用概率的加和规则和乘积规则。我们稍后会看到，这需要对所有$\boldsymbol{w}$值进行积分。对于模式识别来说，这种积分是贝叶斯方法的核心。在曲线拟合问题中，我们知道训练数据$\boldsymbol{x}$和$\boldsymbol{t}$，以及一个新的测试点$x$，我们的目标是预测$t$的值。因此我们想估计预测分布$p(t|x,\boldsymbol{x},\boldsymbol{t})$。这里我们要假设参数$\alpha$和$\beta$是固定的。贝叶斯方法就是自始至终地使用概率的加和规则和乘积规则。因此预测概率可以写成下面的形式：
$$
p(t|x,\boldsymbol{x},\boldsymbol{t})=\int p(t|x,\boldsymbol{w})p(\boldsymbol{w}|\boldsymbol{x},\boldsymbol{t})d\boldsymbol{w}
$$

### 贝叶斯网络

在图的所有结点上定义的联合概率分布由每个结点上的条件概率分布的乘积表示，每个条件概率分布的条件都是结点的父结点所对应的变量。因此，对于一个有K个结点的图，联合概率为 
$$
p(x)=\prod^{K}_{k=1}p(x_k|pa_k)
$$
其中，$pa_k$表示$x_k$的父结点的集合，$x = \{ x1,...,x_K \}$。这个关键的方程表示有向图模型的联合概率分布的分解(factorization)属性。

#### 多项式回归

回顾贝叶斯多项式拟合模型。这个模型中的随机变量是多项式系数向量$\boldsymbol{w}$和观测数据$\boldsymbol{t}= (t_1, . . . , t_N )^T $。此外，这个模型包含输入数据$\boldsymbol{x}= (x_1, . . . , x_N )^T$ 、噪声方差$\sigma^2$以及表示$\boldsymbol{w}$的高斯先验分布的精度的超参数$\alpha$。所有这些都是模型的参数而不是随机变量。现阶段我们只关注随机变量，我们看到联合概率分布等于先验概率分布$p(\boldsymbol{w})$与$N$个条件概率分布$p(t_n |\boldsymbol{w})$的乘积$(n=1,...,N)$，即
$$
p(\boldsymbol{t},\boldsymbol{w}|\boldsymbol{x},\alpha,\sigma^2)=p(\boldsymbol{w}|\alpha)\prod^N_{n=1}p(t_n|\boldsymbol{w},x_n,\sigma^2)
$$
我们通常将某些随机变量设置为具体的值，例如将变量$\{t_n\}$根据多项式曲线拟合中的训练集进行设置。观测到了$\{t_n\}$的值，如果必要的话，我们可以计算系数$\boldsymbol{w}$的的后验概率。现阶段，我们注意到，这是贝叶斯定理的一个直接应用。
$$
p(\boldsymbol{w}|\boldsymbol{t})=p(\boldsymbol{w})\prod^N_{n=1}p(t_n|\boldsymbol{w})
$$
通常，我们对于$\boldsymbol{w}$这样的参数本身不感兴趣，因为我们的最终目标是对输入变量进行预测。 假设给定一个输入值$\hat{x}$ ，我们想找到以观测数据为条件的对应的$\hat{t}$的概率分布。 概率图模型如下所示：

<img src="/Users/aszzy/Documents/study/note/pictures/algorithm/8.png" height="200px">

以确定性参数为条件，这个模型的所有随机变量的联合分布为
$$
p(\hat{t},\boldsymbol{t},\boldsymbol{w}|\hat{x},\boldsymbol{x},\alpha,\sigma^2)=\Big[\prod^N_{n=1}p(t_n|\boldsymbol{w},x_n,\sigma^2)\Big]p({\boldsymbol{w}|\alpha})p(\hat{t}|\hat{x},\boldsymbol{w},\sigma^2)
$$
然后根据概率的加和规则，对模型参数$\boldsymbol{w}$积分，即可得到$\hat{t}$的预测分布
$$
p(\hat{t}|\hat{x},\boldsymbol{x},\boldsymbol{t},\alpha,\sigma^2)\propto \int p(\hat{t},\boldsymbol{t},\boldsymbol{w}|\hat{x},\boldsymbol{x},\alpha,\sigma^2)d\boldsymbol{w}
$$
其中我们隐式地将$\boldsymbol{t}$中的随机变量设置为数据集中观测到的具体值。

#### 生成式模型

##### 祖先取样

考虑$K$个变量的一个联合概率分布$p(x_1,\cdots,x_K)$，它根据公式(11)进行分解，对应于一个有向无环图。我们假设变量已经进行了排序，从而不存在从某个结点到序号较低的结点的链接。我们的目标是从这样的联合概率分布中取样$\hat{x_1},\cdots,\hat{x_K}$。首先选出序号最小的节点，根据概率分布$p(x_1)$取样，计作$\hat{x_1}$。然后顺序计算每个节点，使得对于节点$n$，按照$p(x_n|pa_n)$进行取样。其中父节点的变量被设置成它们的取样值。一旦我们对最后的变量$x_K$取样结束，我们就达到了根据联合概率分布取样的目标。为了从对应于变量的子集的边缘概率分布中取样，我们简单地取要求结点的取样值，忽略剩余结点的取样值。例如，为了从概率分布$p(x_2, x_4)$中取样，我们简单地对联合概率分布取样，然后保留$\hat{x_2},\hat{x_4}$，忽略剩余的值$\{\hat{x}_{j\neq 2,4}\}$。

对于概率模型的实际应用，通常的情况是，数量众多的变量对应于图的终端结点(表示观测值)，较少的变量对应于潜在变量。潜在变量的主要作用是使得观测变量上的复杂分布可以表示为由简单条件分布(通常是指数族分布)构建的模型。  图模型描述了生成观测数据的一种因果关系(causal)过程。因此这种模型通常被称为生成式模型(generative model)。相反，多项式回归模型不是生成式模型，因为没有与输入变量$x$相关联的概率分布，因此无法从这个模型中人工生成数据点。通过引入合适的先验概率分布$p(x)$，我们可以将模型变为生成式模型，代价是增加了模型的复杂度。




